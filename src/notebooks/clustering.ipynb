{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525236b5-051e-45f3-b849-dece7470e076",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "- Creating groups out of data points based on point similarity\n",
    "- Dividing the data into clusters can be on the basis of centroids, distributions, densities, etc\n",
    "\n",
    "*notebook adapted from [Zain Hasan](https://drive.google.com/drive/folders/1s3irmcInqppiKI18rXIY-pnFRbSwd7jq) and James Bain* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9225e03-50f8-4972-be24-79f5c43837d2",
   "metadata": {},
   "source": [
    "## <u>1. K-Means Clustering</u>\n",
    "\n",
    "1. The motivation and purpose of the technique \n",
    "2. Explanation of the k-means clustering \n",
    "3. Shortcomings of the Algorithm\n",
    "4. Show example code\n",
    "5. Show Real life example (digits or image compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e9ef4-b8c0-4104-b176-5e06cf61faa1",
   "metadata": {},
   "source": [
    "**BUT FIRST**\n",
    "\n",
    "![unidentified cluster plot](../images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6b4e9-8e10-4a1d-a688-c86132e3e9dc",
   "metadata": {},
   "source": [
    "**We (humans) are pretty good at identifying patterns in data**\n",
    "\n",
    "![k-means](../images/2.PNG)\n",
    "\n",
    "Can we develop methods for machines to do the same? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf983a3-5ac0-4693-8724-f75128e9c38f",
   "metadata": {},
   "source": [
    "![grouped cluster plot](../images/3.PNG)\n",
    "\n",
    "# K-Means Clustering\n",
    "\n",
    "**K-Means Clustering** is a *unsupervised* clustering machine learning algorithm. It finds clusters in data based on the data attributes alone (not the labels).\n",
    "\n",
    "K-Means searches for cluster centers which are the mean of the points within them, such that every point is closest to the cluster center it is assigned to.\n",
    "\n",
    "![k-means algorithm plots](../images/4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3898b-c101-497f-8f84-a525b53c0652",
   "metadata": {},
   "source": [
    "Visualizing K-means stepping through to find the ideal clusters\n",
    "\n",
    "![kmeans steps](../images/kMeans.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c353d15-8944-4b5a-840c-3dcbbdec52f0",
   "metadata": {},
   "source": [
    "### How KMeans works - Interactive example\n",
    "Go to the link and try choosing centroids to see how kmeans works visually: https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\n",
    "**Follow-up Question: Was it easy to decide clusters always?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50298a8-f79b-4a48-a721-14495ada5035",
   "metadata": {},
   "source": [
    "### <u>Some Short-comings (things to consider) of the algorithm:</u>\n",
    "\n",
    "1. The convergence of this algorithm is not guaranteed; for that reason, scikit-learn by default uses a large number of random initializations and finds the best results.\n",
    "\n",
    "2. Also, the number of clusters must be set beforehand... there are other clustering algorithms for which this requirement may be lifted. - We will see other algorithms that do not need this specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95642c44-d33d-4589-97fc-516d2b9e67f8",
   "metadata": {},
   "source": [
    "#### Let's give it a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6226667-3e54-460d-ba89-133a80926124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans   #<---- We will use sci-kit learns implementation of K-means\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97a54c-b139-4986-ae4c-aba64c311561",
   "metadata": {},
   "source": [
    "### Generate data to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c152e-0dd2-4cf1-8ada-6924b00cc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples = 3000,  # number of datapoints to generate\n",
    "    centers = 4, # How many cluster centers\n",
    "    cluster_std  = 0.60, # Standard deviation for each cluster\n",
    "    random_state = 0 # Set seed so clusters are same for everyone\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391d2ee-77b7-47ca-91e4-977aa1bcd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the generated data\n",
    "plt.scatter(X[:,0], X[:,1], s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484c02d-afb1-4cae-8770-519d960abf60",
   "metadata": {},
   "source": [
    "#### Build model for prediction\n",
    "There are 4 clusters, so we will build a model for 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c719e4-f318-420d-a98b-d12e66593111",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301f093-0db3-4751-a68d-c4fd58df65dd",
   "metadata": {},
   "source": [
    "Train the model off of our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00ede72-1cd0-4c11-a1b1-75b44ff9000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X)   #This is where the EM algorithm is iterating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b122c1-7030-4b43-abdb-409498af6d7d",
   "metadata": {},
   "source": [
    "Predict the clusters for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa6c6b-a3bd-4eda-8f4f-de2c9e70ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa59216-340a-4542-9f9d-6954e6d50c94",
   "metadata": {},
   "source": [
    "Finally, let's plot the predicted clusters along with their centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ffea7-5da7-4c3e-a942-a1420961bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y_pred, s=5)\n",
    "plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc49e1-0ac7-4a49-b102-1889d7cf3148",
   "metadata": {},
   "source": [
    "## Example 1: Digits\n",
    "For a closer-to-real-world example, let's take a look at the digits data. Here we'll use KMeans to automatically cluster the data in 64 dimensions, and then look at the cluster centers to see what the algorithm has found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cedd7d-17c0-44ba-b3b2-9a8e6badac0e",
   "metadata": {},
   "source": [
    "### Import digit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d06ed-e7ef-4418-af00-58bfad56b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "print(\"Number of Data Points:\",digits.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e058635-669c-4831-8cf9-b47acc9efc8a",
   "metadata": {},
   "source": [
    "#### View sample data\n",
    "The data that we imported is a set of images of written numbers. The data is provided in 64 bit vector format, and must be reshaped to 8x8 to be properly viewed as an image. Below we can see 10 random number images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d81444-7ea9-45cd-a8a0-1b9f9b176adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "num_samples = len(digits.data)\n",
    "for i in range(10):\n",
    "    indx = np.random.randint(0, num_samples) # grab a random index\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[]) ## add a subfigure\n",
    "    ax.imshow(digits.data[indx].reshape((8,8)), cmap=plt.cm.binary) # reshaping image to 8x8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851eac5-dc68-4f06-9b40-7f1dd5d805d4",
   "metadata": {},
   "source": [
    "#### Build model\n",
    "Here we will build a `sklearn KMeans` model with 10 possible clusters (0-9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2caaf-f96f-4003-b5ee-c5b8476471e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d4b82-12cd-489c-8426-4827690e19ec",
   "metadata": {},
   "source": [
    "train and predict in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9e1f1-18a5-40c9-9fd8-84a729ccabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = model.fit_predict(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3b673-95f6-4077-9dbd-c0711fe2b791",
   "metadata": {},
   "source": [
    "### Evaluate generated clusters\n",
    "\n",
    "shape of clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a1123-67de-42cb-a713-637f431767c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fad2df-b7e9-4068-83f6-3e53ba042e93",
   "metadata": {},
   "source": [
    "We see that there are 10 clusters in 64 dimensions. \n",
    "\n",
    "#### Visualize clusters\n",
    "Let's visualize each of these cluster centers to see what they represent. The 64 size vector is reshaped into a 8x8, and visualized using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2a97a-f38e-49b0-84ed-1198e0d52cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    ax.imshow(model.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf2abe-aea9-4544-980f-e58220d841ad",
   "metadata": {},
   "source": [
    "From above we can see that even *without labels*, KMeans is able to find clusters whose mean are recognizable as digits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0babfc-b735-41b5-b328-d11ac6e1b494",
   "metadata": {},
   "source": [
    "## Example 2: Colour Compression - AKA the Bob Ross Example\n",
    "One interesting application of clustering is in color image compression. For example, imagine you have an image with millions of colors. In most images, a large number of the colors will be unused, and conversely a large number of pixels will have similar or identical colors.\n",
    "\n",
    "Scikit-learn has a number of images that you can play with, accessed through the datasets module. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9795eb-1f9e-4b6e-8a6d-32b2e93903b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "\n",
    "# Show image\n",
    "plt.imshow(china)\n",
    "plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a01c6c-d84f-4600-89c3-217f128e1e75",
   "metadata": {},
   "source": [
    "### Rescale & Flatten image\n",
    "\n",
    "We can see the shape of the image using the `shape` parameter of the the ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d5398-725b-4164-bdb5-17f85e63756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "china.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf268cf-234c-450d-a265-f0570044a32d",
   "metadata": {},
   "source": [
    "The image above is 640 x 427 and has 3 channels corresponding to Red, Green, and Blue (RGB).\n",
    "\n",
    "We want to rescale the colors in this image so they lie between 0 and 1 (normalize), and then reshape the array into a vector for typical scikit-learn style input:\n",
    "\n",
    "This is an 8 bit image so the color values range between 0 and 255.\n",
    "\n",
    "#### Rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d31150-6c64-4872-88ce-70c3c85ecf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = china / 255.0\n",
    "X[0:5,0:5, 0] # show sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14d26d-5a56-4b22-b7ec-2051688d3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1,3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88540c6-f6fd-4ae1-9ef4-10a0fe206b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "427*640   #Now the image exists as a long column vector of RGB channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd31357-8c2e-40d9-85a0-0269fe5a69ed",
   "metadata": {},
   "source": [
    "We now have $427 \\times 640 = 273,280$ points in 3 dimensions.\n",
    "\n",
    "Our task is to use KMeans to compress the $256^3$ colors into a smaller number (say, 64 colors). Basically, we want to find $N_{color}$ clusters in the data, and create a new image where the true input color is replaced by the color of the closest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c5bd2-9c65-4321-829f-58508a33de5f",
   "metadata": {},
   "source": [
    "#### Subset image to decrease runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4c7ba-05e3-405c-b994-ff1cb1b74f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = china[::3, ::3, :] # take every third\n",
    "print (\"New image shape: \",image.shape)\n",
    "plt.imshow(image)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b3dbe-5f3b-41fa-bde8-d53a54cfae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale and flatten image\n",
    "X = (image/255.0).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd017c6-5dcb-41ad-9f68-940dedb90a1c",
   "metadata": {},
   "source": [
    "#### Build KMeans model\n",
    "w/ 5 clusters corresponding to allowable number of colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051aaa1-737b-41ed-851b-9237d936ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_colors = 5\n",
    "model = KMeans(num_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55593b73-d4f2-4254-a7ce-c1f8f9ce7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & predict with  model\n",
    "labels = model.fit_predict(X)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122fee1c-e8e3-45d3-9262-72ad0414714f",
   "metadata": {},
   "source": [
    "#### Get new clustered image with binned colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2886153-2153-4f24-86a3-30dceefbdbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = model.cluster_centers_\n",
    "new_image = colors[labels].reshape(image.shape)\n",
    "new_image = (255.0 * new_image).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a5e7d-b4fd-41ee-9ced-8cef440d2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image shape: {}\".format(X.shape))\n",
    "print(\"cluster shape: {}\".format(colors.shape))\n",
    "print(\"label shape: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa77e9-164e-49fd-a476-482b8e3901f9",
   "metadata": {},
   "source": [
    "**And let's visualize agains the original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d459ae7-e319-4f4d-8cfe-e6b56c7c7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and plot the new image\n",
    "with sns.axes_style('white'):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title('input')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(new_image)\n",
    "    plt.title('{0} colors'.format(num_colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0ff87-35ab-4988-bd99-a6296abf1918",
   "metadata": {},
   "source": [
    "## 2. Hierchical Clustering\n",
    "In the dendrogram, each leaf corresponds to one object. As we move\n",
    "up the tree, objects that are similar to each other are combined into branches, which\n",
    "are themselves fused at a higher height.\n",
    "\n",
    "The height of the fusion, provided on the vertical axis, indicates the (dis)similarity/distance\n",
    "between two objects/clusters. The higher the height of the fusion, the less similar the\n",
    "objects are\n",
    "\n",
    "One of the problems with hierarchical clustering is that, it does not tell us how many\n",
    "clusters there are, or where to cut the dendrogram to form clusters\n",
    "\n",
    "#### Technique: \n",
    "<img src=\"../images/singlelink1.jpg\"/>\n",
    "<img src=\"../images/singlelink2.jpg\"/>\n",
    "<img src=\"../images/dendogram1.jpg\"/>\n",
    "\n",
    "[source: Toward Data Science](https://towardsdatascience.com/machine-learning-algorithms-part-12-hierarchical-agglomerative-clustering-example-in-python-1e18e0075019)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08b6e3-b86c-462d-a32a-eb58b47e0400",
   "metadata": {},
   "source": [
    "### Linkage Criteria\n",
    "How are you going to calculate distance between two smallest clusters and group them\n",
    "\n",
    "### Types of Linkage\n",
    "<img src=\"../images/singlelinkdist.jpg\"/>\n",
    "<img src=\"../images/completelinkdist.jpg\"/>\n",
    "<img src=\"../images/avglinkdist.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb87e1-3dac-4ae4-9b60-05a1f232f7ac",
   "metadata": {},
   "source": [
    "## Clustering Demo using <code>iris</code> dataset\n",
    "<img src=\"../images/iris.jfif\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d6be9-305b-4ebd-91ba-c4003578ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# check out the first 10 observations\n",
    "iris.data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf055ae1-f2d6-4d67-8d27-a509f6d7cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkout the targets\n",
    "iris.target #hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89691e1-7f9d-48f4-995d-ef4419aab2ba",
   "metadata": {},
   "source": [
    "Now let's compare models when under different linkage criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce486d-d73d-4545-92ca-b4ca54cb1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Hierarchical clustering\n",
    "# single\n",
    "single = AgglomerativeClustering(n_clusters=3, linkage=\"single\")\n",
    "single_pred = single.fit_predict(iris.data)\n",
    "\n",
    "# complete\n",
    "complete = AgglomerativeClustering(n_clusters=3, linkage=\"complete\")\n",
    "complete_pred = complete.fit_predict(iris.data)\n",
    "\n",
    "# average\n",
    "avg = AgglomerativeClustering(n_clusters=3, linkage=\"average\")\n",
    "avg_pred = avg.fit_predict(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa1b0b-2d3d-4559-b723-46be9f8b6ee0",
   "metadata": {},
   "source": [
    "To determine which clustering result better matches the original labels of the samples, we can use ```adjusted_rand_score``` which is an *external cluster validation index* which results in a score between -1 and 1, where 1 means two clusterings are identical of how they grouped the samples in a dataset (regardless of what label is assigned to each cluster).\n",
    "\n",
    "source: Udacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959dcb45-acaf-4cb8-aeae-6812a4f33100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "single_ar_score = adjusted_rand_score(iris.target, single_pred)\n",
    "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\n",
    "avg_ar_score = adjusted_rand_score(iris.target, avg_pred)\n",
    "\n",
    "print( \"Scores: \\nSingle:\", single_ar_score,\"\\nComplete: \", complete_ar_score, \"\\nAverage: \", avg_ar_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2b76a-ad3f-4039-a5b3-8a5c578b5058",
   "metadata": {},
   "source": [
    "To visualize the cluster result we will use Scipy's [```linkage```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) function to perform the clusteirng again so we can obtain the linkage matrix it will later use to visualize the hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a9782-07a1-45d7-b915-ed942735369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy's linkage function to conduct the clustering\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Specify the linkage type. Scipy accepts 'ward', 'complete', 'average', as well as other values\n",
    "# Pick the one that resulted in the highest Adjusted Rand Score\n",
    "linkage_type = 'average'\n",
    "\n",
    "linkage_matrix = linkage(iris.data, linkage_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb0765-0581-4f48-9223-35bcff0802fd",
   "metadata": {},
   "source": [
    "#### Plot <code>linkage</code> matrix using <code>dendogram</code> in <code>scipy</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54049780-2558-43e5-a82f-500167ef576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(22,18))\n",
    "\n",
    "# plot using 'dendrogram()'\n",
    "dendrogram(linkage_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a8737-1450-4e13-a152-5235cc3e6acd",
   "metadata": {},
   "source": [
    "## 3. DBScan(Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "## What does DBScan allows us to do?\n",
    "\n",
    "<img src='../images/dbscanmotivation.PNG' width=\"800\"/>\n",
    "\n",
    "\n",
    "* DBSCAN is a nice alternative to k-means when you don't know how many clusters to expect in your data, but you do know something about how the points should be clustered in terms of density (distance between points in a cluster).\n",
    "\n",
    "* DBSCAN datapoints do not have to be spatial data; they can be color data, intensity values, or other numerical features! This means we can cluster not only based upon proximity, but we can cluster similarly colored objects!\n",
    "\n",
    "\n",
    "[source](https://github.com/chriswernst/dbscan-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7402e07-be3b-4552-b979-a843aac12e9e",
   "metadata": {},
   "source": [
    "### How DBScan Works?\n",
    "\n",
    "\n",
    "Consider a set of points in some space to be clustered. Let **R be a parameter specifying the radius of a neighborhood with respect to some point** and let **M be minimum number of points we want in a neighbourhood to define a cluster**. \n",
    "\n",
    "<img src='../images/DBScan.PNG' width=\"800\"/>\n",
    "\n",
    "\n",
    "For the purpose of DBSCAN clustering, the points are classified as **core points**, **border points** and **outliers**, as follows:\n",
    "\n",
    "* A point p is a **core point** if at least M points are within distance R of it (including p).\n",
    "* A point s is a **border point** if it is near a core point but doesn't have M points in its R neighbourhood.\n",
    "* A point q is **directly density-reachable** from p if point q is within distance R from core point p. Points are only said to be directly reachable from core points.\n",
    "* A point t is **density-reachable** from p if there is a path p1, ..., pn with p1 = p and pn = t, where each pi+1 is directly reachable from pi. Note that this implies that the initial point and all points on the path must be core points, with the possible exception of t. In the image below point t is density reachable from point p via point q.\n",
    "* All points in this chain path p1, ..., pn are called **density connected**.\n",
    "* All points not reachable from any other point are **outliers or noise points**.\n",
    "\n",
    "Now if p is a core point, then it forms a cluster together with all points (core or non-core) that are reachable from it. Each cluster contains at least one core point; non-core points can be part of a cluster, but they form its \"edge\", since they cannot be used to reach more points.\n",
    "\n",
    "<img src='https://www.mdpi.com/applsci/applsci-09-04398/article_deploy/html/images/applsci-09-04398-g001.png' />\n",
    "\n",
    "### Advantages of DBScan\n",
    "<img src='../images/DBScanPros.PNG' />\n",
    "[source: Wikipedia]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01df1d5-448b-48f7-8cc6-02dff7b64500",
   "metadata": {},
   "source": [
    "## Interactive Demo: \n",
    "- https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\n",
    "\n",
    "<img src='../images/dbscansmile.PNG' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d10a9-f19d-45a7-8529-690f868784a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793c452-da47-48ef-b06c-41b14420fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.4, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4516156-263b-4165-8895-34c0f4bc9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "print(colors)\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [242/255, 66/255, 245/255, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01427db-4d40-447a-b1f6-3d4ff2d8fca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
